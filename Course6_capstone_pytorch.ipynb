{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iceQHdrop/Coursera_IBM_AI_engineering/blob/master/Course6_capstone_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6_fl0PWSV1"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpJQRBSLWSV3"
      },
      "source": [
        "\n",
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vbmVvS5WSV4"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxEVkuLEWSV4"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions:\n",
        "\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhfnTKg0WSV4"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_spR7RWSV5"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"https://#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"https://#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"https://#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"https://#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"https://#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"https://#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4VxxBKjWSV5"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeSyY74SWSV5"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI8kiAhWWSV6",
        "outputId": "f1afde10-0f2e-4436-9d0d-ce09d083c82b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-14 10:42:51--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  27.5MB/s    in 89s     \n",
            "\n",
            "2022-06-14 10:44:21 (27.8 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0AVLiNjIWSV6"
      },
      "outputs": [],
      "source": [
        "!unzip -q Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4VqHiQnWSV6",
        "outputId": "4a65536f-4e2b-4b78-b810-6cd8143bd4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-14 10:45:52--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  14.9MB/s    in 2m 32s  \n",
            "\n",
            "2022-06-14 10:48:24 (13.3 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will install torchvision:\n"
      ],
      "metadata": {
        "id": "h7q9wKTmWSV7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uu_0fFN8WSV7",
        "outputId": "b50ab111-aba5-42b2-c63e-58483abccc2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.2.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhhhFfBrWSV7"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbcOBpBcWSV7"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "akV-v8skWSV8",
        "outputId": "4816e628-c6f6-4b32-fba2-a6b56c60e32b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f822c903eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fD2hCEJzWSV8"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X13bk7KWSV9"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERJwTD0iWSV9"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8JfSY5PWSV9"
      },
      "source": [
        "This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Mq0ZBSo-WSV-",
        "outputId": "fad811ac-a704-4af7-8a2d-73b69c5a80b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"./\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SKQTXRUWSV-"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NNAKTaxPWSV_",
        "outputId": "54a36610-384a-4057-f900-bcc8556f4a89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7l_NfAIWSV_"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9geiOv9WSV_"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hmAMzyrWSWA"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dqoNRAKlWSWA",
        "outputId": "2a911fb6-02fd-4ee9-f059-a46fdc81668d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "8d03dafa892040cab7255231fed2333f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d03dafa892040cab7255231fed2333f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "model = models.resnet18(pretrained = True)\n",
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeL4UIgeWSWA"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4JEeAKM5WSWB"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "for para in model.parameters():\n",
        "  para.requires_grad = False\n",
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olv1bv3gWSWB"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSWj36sBWSWB"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "y8950hHBWSWC"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHK2vnfOWSWC"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "9kQZo9hQWSWC",
        "outputId": "e8c063f9-2b36-4dbd-a0bc-fe6954440f0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KLum1WuWSWC"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56trMYFlWSWC"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYxnASAVWSWC"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "sPHYbZDOWSWC"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB5ouxKzWSWC"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "mJpHqZXaWSWD"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = 100)\n",
        "val_loader = DataLoader(validation_dataset, batch_size = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr0jy7CfWSWD"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "H3BCoEjQWSWD"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Yo6tGRWSWD"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'The model will be running at {device} device')"
      ],
      "metadata": {
        "id": "343Byo-IvHan",
        "outputId": "e3777744-2497-43eb-a8f9-ae264428f472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model will be running at cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaF8TCfZWSWD"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "JS8JyA7uWSWD",
        "outputId": "d8ee3aa5-85eb-47ed-a95a-4d35328c3765",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:02<00:00,  1.61it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model.to(device)\n",
        "n_epochs=1\n",
        "# loss_list=[]\n",
        "accuracy_list=[]\n",
        "miss = torch.tensor([]).to(device)\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "n_epochs = 1\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    # for x, y in tqdm(train_loader):\n",
        "    #     x = x.to(device)\n",
        "    #     y = y.to(device)\n",
        "\n",
        "    #     model.train() \n",
        "    #     #clear gradient \n",
        "    #     optimizer.zero_grad()\n",
        "     \n",
        "    #     #make a prediction\n",
        "    #     y_pred = model(x) \n",
        "   \n",
        "    #     # calculate loss\n",
        "    #     loss = criterion(y_pred, y) \n",
        "    #     loss_list.append(loss)\n",
        "    \n",
        "    #     # calculate gradients of parameters\n",
        "    #     loss.backward() \n",
        "        \n",
        "    #     # update parameters \n",
        "    #     optimizer.step()\n",
        "        \n",
        "    #     loss_list.append(loss.data)\n",
        "\n",
        "    correct=0\n",
        "    for x_test, y_test in tqdm(val_loader):\n",
        "        x_test = x_test.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "        # set model to eval\n",
        "        model.eval() \n",
        "       \n",
        "        #make a prediction \n",
        "        y_pred = model(x_test)\n",
        "        \n",
        "        #find max \n",
        "        _, y_hat = torch.max(y_pred, axis = 1)\n",
        "\n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        correct += (y_hat == y_test).sum().item()\n",
        "        miss = torch.cat([miss, y_test[y_hat != y_test]])\n",
        "        \n",
        "   \n",
        "    accuracy=correct/N_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbhNOy1_WSWD"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "0agS_ELpWSWD",
        "outputId": "91dec7b5-469e-41e7-c422-0504937ac1b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9932"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "z589t0gjWSWD",
        "outputId": "7439f3ad-81dc-4c83-bbce-d52efdabbce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "loss_list_plot = [i.cpu().detach().numpy() for i in loss_list]\n",
        "plt.plot(loss_list_plot)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c-wAnkgWSWE"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7O7eKYrWSWE"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "GaSkQSBCWSWE",
        "outputId": "56765293-c63a-4543-e38b-35bd073eec82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 0 predicted value: 1.0 actual value:0.0\n",
            "sample 1 predicted value: 1.0 actual value:0.0\n",
            "sample 2 predicted value: 1.0 actual value:0.0\n",
            "sample 3 predicted value: 1.0 actual value:0.0\n"
          ]
        }
      ],
      "source": [
        "miss_true = 1 - miss\n",
        "for i in range(4):\n",
        "    print(f'sample {i} predicted value: {miss[i]} actual value:{miss_true[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQwW7pOKWSWE"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhmhrNUpWSWE"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2e781tLWSWE"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul0t6u7UWSWE"
      },
      "source": [
        "Copyright © 2018 <a href=\"https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "4.1_resnet18_PyTorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d03dafa892040cab7255231fed2333f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_485b1c040d5d40e4a1c27b1fbff3a385",
              "IPY_MODEL_2eb345f890b146bb93f9db9804af1b9d",
              "IPY_MODEL_1d1c8750a49b471ebd06846629786976"
            ],
            "layout": "IPY_MODEL_a67634843f5b4d9da98c5d768e403b61"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}